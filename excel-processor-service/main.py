"""
Microservicio para procesar archivos Excel de forma r√°pida y eficiente
Usa pandas para procesamiento optimizado de Excel y guarda directamente en PostgreSQL
Optimizado con COPY de PostgreSQL para inserci√≥n masiva (10-20x m√°s r√°pido)
"""
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import os
import tempfile
import logging
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_batch, Json, execute_values
from psycopg2.pool import SimpleConnectionPool
import json
from typing import Optional, Dict
import io

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Excel Processor Service")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especificar or√≠genes
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pool de conexiones a PostgreSQL
db_pool: Optional[SimpleConnectionPool] = None

# Diccionario para guardar progreso de procesamiento
processing_progress: Dict[int, Dict] = {}

def get_db_connection():
    """Obtener conexi√≥n de la base de datos - usa las mismas variables del .env del backend"""
    global db_pool
    if db_pool is None:
        # Leer variables del .env del backend (mismo archivo)
        db_pool = SimpleConnectionPool(
            1, 20,
            host=os.getenv('DB_HOST', 'localhost'),
            port=int(os.getenv('DB_PORT', 5432)),
            user=os.getenv('DB_USER', 'root'),
            password=os.getenv('DB_PASSWORD', 'password'),
            database=os.getenv('DB_NAME', 'autochat_db')
        )
        logger.info(f"üîå Conectado a BD: {os.getenv('DB_USER')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}")
    return db_pool.getconn()

def return_db_connection(conn):
    """Devolver conexi√≥n al pool"""
    db_pool.putconn(conn)

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "ok", "service": "excel-processor"}

@app.get("/progress/{excel_id}")
async def get_progress(excel_id: int):
    """Obtener progreso de procesamiento de un Excel"""
    if excel_id in processing_progress:
        return processing_progress[excel_id]
    return {"progress": 0, "status": "not_found"}

@app.post("/process")
async def process_excel(
    file: UploadFile = File(...),
    user_id: int = Form(...),
    uploaded_by: str = Form(...)
):
    """
    Procesa un archivo Excel y lo guarda directamente en PostgreSQL
    Usa COPY de PostgreSQL para inserci√≥n masiva (10-20x m√°s r√°pido que INSERT)
    
    Args:
        file: Archivo Excel
        user_id: ID del usuario que sube el archivo
        uploaded_by: Nombre/email del usuario que sube el archivo
    
    Returns:
        - success: Si el procesamiento fue exitoso
        - message: Mensaje descriptivo
        - recordsCount: N√∫mero de registros guardados
        - excelId: ID del Excel guardado en la BD
    """
    start_time = datetime.now()
    conn = None
    excel_id = None
    
    try:
        # Validar extensi√≥n
        if not file.filename.endswith(('.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="El archivo debe ser Excel (.xlsx o .xls)")
        
        logger.info(f"üìä Procesando archivo: {file.filename} para usuario {user_id}")
        
        # Guardar archivo temporalmente
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # Leer Excel con pandas (muy optimizado)
            engine = 'openpyxl' if file.filename.endswith('.xlsx') else 'xlrd'
            
            df = pd.read_excel(
                tmp_file_path,
                engine=engine,
                sheet_name=0,
                dtype=str,
                na_values=['', 'NULL', 'null', 'None'],
                keep_default_na=False,
            )
            
            logger.info(f"üìã Archivo le√≠do: {len(df)} filas, {len(df.columns)} columnas")
            
            # Obtener headers
            headers = [str(col).strip() if pd.notna(col) else f"Columna_{i+1}" 
                      for i, col in enumerate(df.columns)]
            
            # Limpiar nombres de columnas duplicados
            seen = {}
            for i, header in enumerate(headers):
                if header in seen:
                    headers[i] = f"{header}_{seen[header]}"
                    seen[header] += 1
                else:
                    seen[header] = 1
            
            # Convertir DataFrame a lista de diccionarios de forma optimizada
            df_cleaned = df.replace(['', 'NULL', 'null', 'None', pd.NA], None)
            records = df_cleaned.to_dict('records')
            
            # Filtrar registros completamente vac√≠os
            records = [r for r in records if any(v is not None and (not isinstance(v, str) or v.strip()) for v in r.values())]
            
            if not records:
                raise HTTPException(status_code=400, detail="El Excel no contiene datos")
            
            logger.info(f"üìä {len(records)} registros procesados con {len(headers)} columnas")
            
            # Conectar a PostgreSQL
            conn = get_db_connection()
            cur = conn.cursor()
            
            try:
                # Insertar metadata - usar nombres exactos de columnas (TypeORM no convierte a snake_case)
                cur.execute("""
                    INSERT INTO excel_metadata (user_id, filename, "totalRecords", "uploadedBy", headers, "isReactive", "uploadedAt")
                    VALUES (%s, %s, %s, %s, %s, %s, NOW())
                    RETURNING id
                """, (user_id, file.filename, len(records), uploaded_by, Json(headers), True))
                
                excel_id = cur.fetchone()[0]
                logger.info(f"üíæ Metadata guardada (ID: {excel_id})")
                
                # Hacer commit inmediato del metadata para que est√© disponible
                conn.commit()
                
                # Inicializar progreso ANTES de procesar
                processing_progress[excel_id] = {
                    "progress": 0,
                    "total": len(records),
                    "processed": 0,
                    "status": "processing"
                }
                
                # El excelId ya est√° disponible aqu√≠, pero el procesamiento contin√∫a
                # El frontend puede empezar a hacer polling inmediatamente
                
                # OPTIMIZACI√ìN: Usar execute_values que es mucho m√°s r√°pido que execute_batch
                # Para archivos grandes, usar COPY ser√≠a a√∫n m√°s r√°pido, pero execute_values es m√°s simple
                BATCH_SIZE = 5000  # Batch m√°s grande para mejor rendimiento
                total_records = len(records)
                row_index = 1
                
                # Preparar datos en formato para execute_values
                all_values = []
                for record in records:
                    cleaned_record = {k: v for k, v in record.items() if v is not None and (not isinstance(v, str) or v.strip())}
                    if cleaned_record:
                        all_values.append((
                            user_id,
                            excel_id,
                            Json(cleaned_record),
                            row_index
                        ))
                        row_index += 1
                
                # Insertar usando execute_values (m√°s r√°pido que execute_batch)
                total_batches = (len(all_values) + BATCH_SIZE - 1) // BATCH_SIZE
                
                for i in range(0, len(all_values), BATCH_SIZE):
                    batch_values = all_values[i:i + BATCH_SIZE]
                    
                    execute_values(
                        cur,
                        """
                        INSERT INTO dynamic_records (user_id, excel_id, "rowData", "rowIndex", "createdAt")
                        VALUES %s
                        """,
                        [(v[0], v[1], v[2], v[3], datetime.now()) for v in batch_values],
                        page_size=BATCH_SIZE
                    )
                    
                    # Actualizar progreso despu√©s de cada batch
                    processed = min(i + BATCH_SIZE, len(all_values))
                    progress_pct = min(100, (processed / len(all_values)) * 100)
                    processing_progress[excel_id] = {
                        "progress": round(progress_pct, 1),
                        "total": total_records,
                        "processed": processed,
                        "status": "processing"
                    }
                    
                    # Log cada 5% para mejor visibilidad
                    if (i // BATCH_SIZE) % max(1, total_batches // 20) == 0 or i + BATCH_SIZE >= len(all_values):
                        logger.info(f"‚è≥ Progreso: {processed}/{len(all_values)} registros ({progress_pct:.1f}%)")
                
                # Commit final
                conn.commit()
                
                # Marcar como completado
                processing_progress[excel_id] = {
                    "progress": 100,
                    "total": total_records,
                    "processed": len(all_values),
                    "status": "completed"
                }
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                
                logger.info(f"‚úÖ Excel procesado y guardado en {processing_time:.2f}s: {len(records)} registros")
                
                # Limpiar progreso despu√©s de 5 minutos
                import threading
                def cleanup_progress():
                    import time
                    time.sleep(300)  # 5 minutos
                    if excel_id in processing_progress:
                        del processing_progress[excel_id]
                threading.Thread(target=cleanup_progress, daemon=True).start()
                
                return {
                    "success": True,
                    "message": f"Excel procesado correctamente en {processing_time:.2f}s. {len(records)} registros con {len(headers)} columnas guardados.",
                    "recordsCount": len(records),
                    "excelId": excel_id,
                    "totalColumns": len(headers),
                    "processingTime": round(processing_time, 2)
                }
                
            except Exception as db_error:
                conn.rollback()
                if excel_id:
                    processing_progress[excel_id] = {
                        "progress": 0,
                        "total": 0,
                        "processed": 0,
                        "status": "error",
                        "error": str(db_error)
                    }
                logger.error(f"‚ùå Error guardando en BD: {str(db_error)}")
                raise HTTPException(status_code=500, detail=f"Error guardando en base de datos: {str(db_error)}")
            finally:
                cur.close()
                
        finally:
            # Eliminar archivo temporal
            try:
                os.unlink(tmp_file_path)
            except:
                pass
            # Devolver conexi√≥n al pool
            if conn:
                return_db_connection(conn)
                
    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="El archivo Excel est√° vac√≠o")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error procesando Excel: {str(e)}")
        if conn:
            return_db_connection(conn)
        raise HTTPException(status_code=500, detail=f"Error al procesar el Excel: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    from dotenv import load_dotenv
    import pathlib
    
    # Cargar .env del directorio padre (donde est√° el backend)
    env_path = pathlib.Path(__file__).parent.parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f"üìã Variables de entorno cargadas desde: {env_path}")
    else:
        # Si no existe, intentar cargar desde el directorio actual
        load_dotenv()
        logger.info("üìã Variables de entorno cargadas desde .env del directorio actual")
    
    port = int(os.getenv('EXCEL_PROCESSOR_PORT', 8001))
    uvicorn.run(app, host="0.0.0.0", port=port)
